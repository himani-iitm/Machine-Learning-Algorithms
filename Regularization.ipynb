{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-guide",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Homework 1, PHY 959 -->\n",
    "<!-- dom:AUTHOR: [PHY 959: Machine Learning in Physics]-->\n",
    "\n",
    "\n",
    "# PHY 905: Homework Set #2\n",
    "Due: **February 2, 2023**\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "***\n",
    "\n",
    "\n",
    "# Problem 2: Exploring Ensembles\n",
    "\n",
    "In this problem, we will exercise what we learned about using ensembles to improve the reliability of our training.  We will continue to use sparse datasets with one dimension, as it's easier to visualize.  However, these methods will be useful in the rest of the course material...especially with neural networks and boosted decision trees.  \n",
    "\n",
    "Parts 2a-2c will be almost identical to what you have already seen in class or homework, so use what you can from those and copy/paste.  That will not be considered plagiarism.\n",
    "\n",
    "There is a lot of \"starter\" code here that will not work out of the box.  You'll need to add your own code to fill in some gaps that are missing.  Ask Prof. Fisher if you get stuck.\n",
    "\n",
    "## Part 2a:\n",
    "\n",
    "This is almost exactly the same as what we did for Homework Set 2 Problem 1a, so feel free to copy that code over to the next cell!  Make the following edits:\n",
    "\n",
    "  1. Increase the number of training data points from 10 to 40, but keep the sampling of the x range random.\n",
    "  2. Change the functional form for the targets (```ytrain```) to be $y=0.75+3x-2x^2+2x^3-2x^4+0.3\\sin(x\\times 15)$.  For the training data, add some Gaussian noise (ie, add $+ \\rm{Gaus}(0,0.15)$ to ytrain).\n",
    "  3. Make another set of 20 data points in the range `[0,1]` with the same functional form, but increase the noise to 0.60.  Then concatenate (append) to the previous training set.  This will give you a set of VERY noisy data with outliers that can impact your fits.  As you're doing the appending, recall that `np.random.uniform` gives you a `numpy` array and not a standard Python array.  You need to use `np.append()` for this.\n",
    "  4. I suggest that you make a testing sample with zero noise for comparisons later in the notebook.\n",
    "  5. It will help your visualizations if you do an `argsort` to permute the indices of your appended training arrays.\n",
    "  \n",
    "Keep the testing data as a convenient way to plot the true function.  You shouldn't add the noise to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize the data\n",
    "\n",
    "# Make some training data\n",
    "# First, 40 points that are a little noisy\n",
    "nPts = 40\n",
    "np.random.seed(37)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-occasions",
   "metadata": {},
   "source": [
    "## Part 2b:\n",
    "\n",
    "Create a design/feature matrix for your samples.  Do this in the same way that you did for Problem 1b.  Use a 7th order polynomial (nDegr=7).  Again, I strongly encourage you to try out the module ```PolynomialFeatures``` from ```scikit-learn```.  Don't forget your randomized training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stone-voltage",
   "metadata": {},
   "source": [
    "## Part 2c:\n",
    "\n",
    "Use the Numpy ```linalg``` module to calculate the vector of weights that best fit your data.  Make a plot of your training data and your best fit line.  Also plot your testing data to illustrate how well the linear regression fit reproduces your underlying true function.\n",
    "\n",
    "A convenient way to generate your line would be like this:  ```line = np.dot(DM,betas)``` wherein ```betas``` is the weights vector from your ```linalg``` solution.  Just like Problem #1c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-moore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vital-chess",
   "metadata": {},
   "source": [
    "## Part 2d:\n",
    "\n",
    "Here we will explore two strategies for generating mini-batches for performing cross-validation training.  \n",
    "1. The first strategy will use k-fold cross validation.  This is performed by breaking your data sample into k orthogonal subsamples and training on each subsample independently.  \n",
    "2. The second strategy will be to generate bootstrap data samples, wherein the training data are randomly sampled (with replacement).  \n",
    "    \n",
    "In each case, you will use a range of mini-batch sizes, ranging from small (10%) to big (50%).  The way you will use these mini-batches is to fit your polynomial function to each mini-batch and average over all mini-batches to find an aggregate \"best fit\".  For example, if you have a mini-batch size of 10%, you will have 10 mini-batches and, thus, 10 regression results to average.\n",
    "\n",
    "To avoid spending your time in Python developer mode, most of the code has been provided.  Your job will be to create functions that properly generate the mini-baches for training.  Do not use ```scikit-learn``` for this, as the goal is to generate your own intuition for how the algorithm should work.  However, the use of ```Numpy``` is stronly encouraged.  You can either set up your code to perform both k-fold cross validation and bootstrap sampling at once, or allow toggling between the two. **If you get stuck, consult a classmate or Prof. Fisher!**\n",
    "\n",
    "Some things to think about:\n",
    "\n",
    "  1. When you make your mini-batches, you need to provide subsamples for both the design matrix and for the targets.\n",
    "  2. Your mini-batches must maintain the one-to-one relationship of design matrix rows to targets.  This will be more challenging with the bootstrap random sampling.  I suggest exploring the ```Numpy::random.choice()``` method.\n",
    "\n",
    "\n",
    "You will need to make plots of your results in the next part of the problem.  So you need two arrays to keep track of your results:\n",
    "\n",
    "  1. An array to average the fit parameters from each mini-batch.  For example, if you choose mini-batch size to be 10%, you will have 10 mini-batches to average over.  This array should be extended to hold all of your mini-batch size averages.  The size should be ```(batchRange.size,nDegr+1)```.\n",
    "  2. An array to hold the weights for fits to each of the mini-batches in your smallest mini-batch size.  This will be to illustrate the power of averaging over mini-batches.  The size should be ```(nBatches,nDegr+1)```.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 0.2  # Note that if the learning rate is too big, you may not converge!\n",
    "\n",
    "step = 0.1  # number of steps for the batch size to be tested over\n",
    "bMin = 0.1\n",
    "bMax = 0.501\n",
    "\n",
    "doRandomBatch = 1\n",
    "\n",
    "# this array will hold the values of the regularization strength that we'll test\n",
    "batchRange = np.arange(bMin,bMax+0.001,step)\n",
    "\n",
    "# this array will hold the results of our various fits\n",
    "weights = np.zeros((batchRange.size,nDegr+1))\n",
    "\n",
    "# this array will hold the results of our smallest batch size\n",
    "smallWeights = np.zeros((np.int(0.01+1.0/batchRange[0]),nDegr+1))\n",
    "\n",
    "# gradient descent loop\n",
    "Niter = 2500\n",
    "for bidx, batchFrac in enumerate(batchRange):\n",
    "\n",
    "    #calculate the number of batches we can get using this fraction\n",
    "    batchIterations = np.int(0.01+1.0/batchFrac)\n",
    "    avgWeights = np.zeros(betas.size)\n",
    "    \n",
    "    print(\"Batch Size Fraction:\",batchFrac)\n",
    "    for aidx in range(batchIterations):\n",
    "        \n",
    "        # build a mini-batch!\n",
    "        if doRandomBatch: # bootstrap data samples\n",
    "            iDM, iy = ?? # function to create DM and targets for bootstrap sample of the right size\n",
    "        else: # k-fold cross validation data samples\n",
    "            iDM, iy = ?? # function to create DM and targets for k-fold cross validation of the right size\n",
    "        \n",
    "    \n",
    "        #Start from our analytical fit to speed convergence\n",
    "        W = betas*0.99\n",
    "        for i in range(Niter):\n",
    "  \n",
    "            # evaluate function values\n",
    "            fhat = np.dot(iDM,W)\n",
    "\n",
    "            # compute mean squared loss\n",
    "            data_loss = np.sum((iy-fhat)**2)\n",
    "            data_loss /= iy.size\n",
    "\n",
    "            # Report progress\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Batch %.3f, iteration %d: loss %f\" % (batchFrac,i, data_loss))\n",
    "        \n",
    "            #compute the loss gradients\n",
    "            dW = -2*np.dot(iDM.T,iy-fhat)\n",
    "            dW /= iy.size\n",
    "                \n",
    "            # update the weights\n",
    "            W -= step_size*dW\n",
    "\n",
    "        #capture weights\n",
    "        avgWeights += W*1.0/batchIterations\n",
    "        if bidx == 0:\n",
    "            smallWeights[aidx] = W\n",
    "        \n",
    "    print(\"Final Weights:\\n\",avgWeights)\n",
    "    weights[bidx] = avgWeights\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-encoding",
   "metadata": {},
   "source": [
    "## Part 2e:\n",
    "\n",
    "Now that you have your mini-batch results, create two figures:\n",
    "\n",
    "  1. A plot of your training and test samples overlaid with all N of the fits from your smallest mini-batch fraction.  For example, if your smallest mini-batch fraction is 10%, you'll have 10 lines overlaid.\n",
    "  2. A plot of your training and test samples overlaid with all M of your mini-batch fit averages.\n",
    "  \n",
    "You should be able to make these figures for both k-fold cross validation and for random bootstrap sampling.  Once you've inspected all four figures, please answer the following questions:\n",
    "\n",
    "  1. Which ensemble method give you the smallest variance between mini-batch sizes?  Why do you think this is?\n",
    "  2. Which ensemble method reproduces the true underlying function the best?  Is that a good thing or not in this case?\n",
    "  3. What are the apparent advantages of large or small mini-batch sizes?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-atlanta",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
