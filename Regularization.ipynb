{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-thriller",
   "metadata": {},
   "source": [
    "## <!-- dom:TITLE: Homework 1, PHY 959 -->\n",
    "<!-- dom:AUTHOR: [PHY 959: Machine Learning in Physics]-->\n",
    "\n",
    "\n",
    "# PHY 905: Homework Set #2\n",
    "Due: **February 6, 2025**\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "***\n",
    "\n",
    "\n",
    "# Problem 1: Exploring Regularization\n",
    "\n",
    "In this problem, we will exercise what we learned about regularization.  We will use two different data sets that allow us to see regularization at work with a linear regression problem.  You will be fitting a polynomial function to data that is not quite linear, and has enough noise to confuse our choice of polynomial function.  And the data will be sparse enough so that overfitting will be a real concern.\n",
    "\n",
    "We have to pretend that we don't know the highest order of the polynomial in the data, so we need to allow for a potentially large polynomial order.  But if we use a high enough polynomial order, we'll overfit our data and lose the ability to predict new data.  The goal will be to learn to identify cases where we've overfit our data and to use regularization to reduce the overfitting.\n",
    "\n",
    "The workflow for this problem will be to:\n",
    "\n",
    "  1. Part 1a: generate testing and training data samples.\n",
    "  2. Part 1b: create a design matrix with polynomial features.\n",
    "  3. Part 1c: use the mean squared error loss to find the analytical solution for the optimal weights.\n",
    "  4. Part 1d: perform a gradient descent fit to the data, changing regularization strength to find the optical choice.\n",
    "  5. Part 1e: plot the parameters of your polynomial fit as a function of regularization strength\n",
    "  6. Part 1f: plot the loss for each fit as a function of regularization strength, relative to your test sample.\n",
    "\n",
    "There is a lot of \"starter\" code here that will not work out of the box.  You'll need to add your own code to fill in some gaps that are missing.  Ask Prof. Fisher if you get stuck.\n",
    "\n",
    "## Part 1a:\n",
    "\n",
    "In the next cell, you are given:\n",
    "\n",
    "  1. A training data sample to use to fit your polynomial\n",
    "  2. An independent testing data sample to evaluate the quality of your fit\n",
    "\n",
    "Make a plot of these two samples.  I suggest to use the `MatPlotLib` scatterplot method on the same graph using two different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize the data\n",
    "\n",
    "# Make some training data\n",
    "nPts = 10\n",
    "np.random.seed(119)\n",
    "Xtrain = np.random.uniform(0,1,nPts)\n",
    "ytrain = 0.1*Xtrain + 1*np.sin(Xtrain*15) \n",
    "\n",
    "# Make some testing data, 3x as many as training\n",
    "# Here we will use the same functional form to \n",
    "# effectively expand the data sample\n",
    "np.random.seed(122)\n",
    "Xtest = np.random.uniform(0,1,nPts*3)\n",
    "ytest = 0.1*Xtest + 1*np.sin(Xtest*15)\n",
    "\n",
    "# Because we're going to be plotting lines, it will be very convenient if\n",
    "# our data is ordered by the absyssa.\n",
    "print(\"Before permutation\\n\",Xtrain)\n",
    "permute = Xtrain.argsort()\n",
    "Xtrain=Xtrain[permute]\n",
    "ytrain=ytrain[permute]\n",
    "print(\"After permutation\\n\",Xtrain)\n",
    "\n",
    "permute = Xtest.argsort()\n",
    "Xtest=Xtest[permute]\n",
    "ytest=ytest[permute]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-encounter",
   "metadata": {},
   "source": [
    "## Part 1b:\n",
    "\n",
    "Create a design/feature matrix for your samples.  In previous examples, the design matrix contained one variable for each dimension. Because we're now doing polynomial regression in one dimension, you will need to create a d-dimensional design matrix where d is the highest polynomial order.  For this problem, use d=7.  You can try out other values, but when you turn in your work please use 7 degrees of freedom.  Note that one of these will be the offset (bias) variable.\n",
    "\n",
    "I strongly encourage you to try out the module ```PolynomialFeatures``` from ```scikit-learn```, which you have seen in our lecture discussion of January 12th.  Note that the variable to be transformed by the ```PolynomialFeatures``` `fit_transform` function needs to be an array of size ```(Npts,1)```.  Print out your design matrix so you can see what the ```PolynomialFeatures``` module does for you.  Don't forget to build a design matrix for your testing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "nDegr = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-discussion",
   "metadata": {},
   "source": [
    "## Part 1c:\n",
    "\n",
    "Using your skills learned from homework #1, use the Numpy ```linalg``` module to calculate the vector of weights that best fit your training data.  Make a plot of your training data and your best fit line.  A convenient way to generate your line would be like this:  ```line = np.dot(DM,betas)``` wherein ```betas``` is the weights vector from your ```linalg``` solution and `DM` is clearly the design matrix.  Add your testing data as well to get a feeling for how good (or bad) the fit on the training data works for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate our betas\n",
    "betas = ??\n",
    "\n",
    "#print out our betas\n",
    "print(betas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-press",
   "metadata": {},
   "source": [
    "## Part 1d:\n",
    "\n",
    "Now let's set up to do a fit of our data and add regularization to the mix.  I've provided a starting point to speed you up.  Note that there is a \"toggle\" to switch between L2 and L1 regularization.  **You'll need to work out the correct regularization loss and gradients for each choice to make the code work properly**.\n",
    "\n",
    "This step fills a matrix (`weights`) that holds the polynomial weights for a range of regularization strengths, `[rMin,rMax]` in 5 steps.\n",
    "\n",
    "In the following, you'll be making some plots of these weights. You do not need to turn in plots using both regularization schemes.  It is enough that the plots can be generated as you toggle between L2 and L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression\n",
    "\n",
    "# Initialize parameters from our fit.  To reduce training time, we may as well\n",
    "# use results close to our previous step! \n",
    "W = betas*0.99\n",
    "\n",
    "# gradient descent step size, this is a fraction of the gradient \n",
    "step_size = 0.1  # Note that if the learning rate is too big, you may not converge!\n",
    "\n",
    "#Toggle between L2 and L1 regularization\n",
    "doL2 = 0\n",
    "\n",
    "# Regularization strength scan parameters\n",
    "Nsteps = 5  # number of steps for the regularization strength to be tested over\n",
    "rMin = 0.0  \n",
    "rMax = 0.1\n",
    "\n",
    "# this array will hold the values of the regularization strength that we'll test\n",
    "regRange = np.arange(rMin,rMax,(rMax-rMin)/Nsteps)\n",
    "\n",
    "# this array will hold the results of our various fits\n",
    "weights = np.zeros((regRange.size,nDegr+1))\n",
    "\n",
    "# gradient descent loop\n",
    "Niter = 10000\n",
    "\n",
    "# Create a loop for the regularization strength\n",
    "for ridx, reg in enumerate(regRange):\n",
    "    #always reset the weights!\n",
    "    W = betas*0.99\n",
    "    \n",
    "    # perform gradient descent with a max of Niter iterations\n",
    "    for i in range(Niter):\n",
    "  \n",
    "        # evaluate function values\n",
    "        fhat = np.dot(DM,W)\n",
    "\n",
    "        # compute mean squared loss\n",
    "        data_loss = np.sum((ytrain-fhat)**2)\n",
    "        data_loss /= nPts\n",
    "\n",
    "        # Regularize!\n",
    "        # L2 & L1 regularization\n",
    "        # Add the correct regularization loss function\n",
    "        if doL2 :\n",
    "            reg_loss = ??\n",
    "        else:\n",
    "            reg_loss = ??\n",
    "\n",
    "        # Total loss is the sum\n",
    "        loss = reg_loss + data_loss\n",
    "\n",
    "        # Report progress\n",
    "        if i % 2000 == 0:\n",
    "            print(\"Reg %.3f, iteration %d: loss %f\" % (reg,i, loss))\n",
    "        \n",
    "        #compute the loss gradients\n",
    "        dW = -2*np.dot(DM.T,ytrain-fhat)\n",
    "        dW /= nPts\n",
    "        \n",
    "        # Gradient descent needs the regularization gradient\n",
    "        # You need to add the correct gradients below\n",
    "        if doL2:\n",
    "            dW += ?? # regularization gradient for L2\n",
    "        else:            \n",
    "            dW += ?? # regularization gradient for L1\n",
    "        \n",
    "        # Update weights after this iteration\n",
    "        # We always correct weights in the negative of the gradient direction,\n",
    "        # which ensure that we step to the minimum\n",
    "        W -= step_size*dW\n",
    "\n",
    "    #capture weights\n",
    "    weights[ridx,:] = W\n",
    "    print(\"Added Weights for Step Index: \", ridx)\n",
    "    \n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-california",
   "metadata": {},
   "source": [
    "## Part 1e:\n",
    "\n",
    "You now have a weights matrix that is of size ```(regRange.size,nDegr+1)```, wherein each row is the weights found for a given regularization strength.  Make two figures using this data:\n",
    "\n",
    "  1. For each polynomial order, create a line corresponding to the associated coefficient as a function of the regularization strength.  Eg, for polynomial order 5, you will have a line with 10 values corresponding to the 10 steps in the regularization strength.  You'll create a total of 8 lines for polynomial order `[0,7]`.\n",
    "  2. Plot the resulting function found for each regularization strength value.  Include the testing and training data in the figure.  You will have 10 total lines, one fore each regularization strength.\n",
    "  \n",
    "The default maximum regularization strength is 0.1.  Is that a sensible value for both L1 and L2 regularization?  **If not, feel free to edit that value until you get fitted values that feel right to you.**\n",
    "  \n",
    "Please use your figures to support answers to the following questions:\n",
    "\n",
    "  1. What do you conclude about your choice of regularization scheme, L2 vs L1?\n",
    "  2. At this point, can you state which regularization strength is \"best\" for each case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-sapphire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faced-lingerie",
   "metadata": {},
   "source": [
    "## Part 1f:\n",
    "\n",
    "Last one!!  To better understand the effect of regularization on this problem, let's study the testing data sample.  Make a final plot that shows the mean squared error loss as a function of regularization strength.  Note that you should be able to reuse a lot of the code from the previous step.  Be careful about finding the mean value, as your testing and training data samples are not the same size!\n",
    "\n",
    "Use this figure and the previous two to answer the following:\n",
    "\n",
    "  1. Which regularization strength appears to be best for both L1 and L2 regularization?\n",
    "  2. For this problem, which regularization scheme appears to work better?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-welsh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
